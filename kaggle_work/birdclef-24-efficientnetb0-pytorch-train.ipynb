{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": true
   },
   "source": [
    "# BirdCLEF 2024 [Train]\n",
    "\n",
    "This is the baseline of EfficientNetB0 with PyTorch. I strive to simplify the process to get started faster. Therefore, I only use data from BirdCLEF 2024 (no extended data from BirdCLEF'23, 22, 21, and other sources) and unlabeled soundscapes are also not used. Besides, PyTorch-Lightning is employed to organize the training.\n",
    "\n",
    "Hope this notebook is useful for you!!\n",
    "\n",
    "* [Pre-Processing](https://www.kaggle.com/code/zijiangyang1116/birdclef-24-speed-up-audio-to-spec-via-cupy)\n",
    "* [The inference Notebook](https://www.kaggle.com/code/zijiangyang1116/birdclef-24-efficientnetb0-pytorch-inference)\n",
    "\n",
    "## Features\n",
    "- Implement with PyTorch and PyTorch-Lightning\n",
    "- Speed up audio-to-spec. via CuPy\n",
    "- Use EfficientNetB0 from torchvision\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Import Packages](#Import-Packages)\n",
    "- [Configuration](#Configuration)\n",
    "- [Dataset & Dataloader](#Dataset-&-Dataloader)\n",
    "- [Model](#Model)\n",
    "- [Functions of Training Loop](#Functions-of-Training-Loop)\n",
    "- [Training](#Training)\n",
    "\n",
    "## Update\n",
    "\n",
    "- V3: fix bug - After validation, self.validation_step_outputs should be cleared.\n",
    "- V4: use XYMasking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "Import all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:50:09.751961Z",
     "iopub.status.busy": "2025-07-03T18:50:09.751599Z",
     "iopub.status.idle": "2025-07-03T18:50:09.759636Z",
     "shell.execute_reply": "2025-07-03T18:50:09.758641Z",
     "shell.execute_reply.started": "2025-07-03T18:50:09.751933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "operator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m efficientnet\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01malbu\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpl\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/complex_net/lib/python3.9/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/complex_net/lib/python3.9/site-packages/torchvision/_meta_registrations.py:164\u001b[0m\n\u001b[1;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         ),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision::nms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should have 4 elements in dimension 1, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/complex_net/lib/python3.9/site-packages/torch/library.py:795\u001b[0m, in \u001b[0;36mregister\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mregister_kernel\u001b[39m(\n\u001b[1;32m    764\u001b[0m     op: _op_identifier,\n\u001b[1;32m    765\u001b[0m     device_types: device_types_t,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m     lib: Optional[Library] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    770\u001b[0m ):\n\u001b[1;32m    771\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Register an implementation for a device type for this operator.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \n\u001b[1;32m    773\u001b[0m \u001b[38;5;124;03m    Some valid device_types are: \"cpu\", \"cuda\", \"xla\", \"mps\", \"ipu\", \"xpu\".\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03m    This API may be used as a decorator.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m        op (str | OpOverload): The operator to register an impl to.\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;124;03m        device_types (None | str | Sequence[str]): The device_types to register an impl to.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;124;03m            If None, we will register to all device types -- please only use\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m            this option if your implementation is truly device-type-agnostic.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m        func (Callable): The function to register as the implementation for\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m            the given device types.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m        lib (Optional[Library]): If provided, the lifetime of this registration\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    Examples::\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03m        >>> import torch\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03m        >>> from torch import Tensor\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m        >>> from torch.library import custom_op\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m        >>> import numpy as np\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;124;03m        >>> # Create a custom op that works on cpu\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;124;03m        >>> @custom_op(\"mylib::numpy_sin\", mutates_args=(), device_types=\"cpu\")\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m        >>> def numpy_sin(x: Tensor) -> Tensor:\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m \u001b[38;5;124;03m        >>>     x_np = x.numpy()\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m        >>>     y_np = np.sin(x_np)\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m        >>>     return torch.from_numpy(y_np)\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;124;03m        >>> # Add implementations for the cuda device\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m        >>> @torch.library.register_kernel(\"mylib::numpy_sin\", \"cuda\")\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03m        >>> def _(x):\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m        >>>     x_np = x.cpu().numpy()\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m        >>>     y_np = np.sin(x_np)\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03m        >>>     return torch.from_numpy(y_np).to(device=x.device)\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;124;03m        >>> x_cpu = torch.randn(3)\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;124;03m        >>> x_cuda = x_cpu.cuda()\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;124;03m        >>> assert torch.allclose(numpy_sin(x_cpu), x_cpu.sin())\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m        >>> assert torch.allclose(numpy_sin(x_cuda), x_cuda.sin())\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    814\u001b[0m         op, (\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_ops\u001b[38;5;241m.\u001b[39mOpOverload, torch\u001b[38;5;241m.\u001b[39m_library\u001b[38;5;241m.\u001b[39mcustom_ops\u001b[38;5;241m.\u001b[39mCustomOpDef)\n\u001b[1;32m    815\u001b[0m     ):\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_kernel(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): got unexpected type for op: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(op)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    818\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/complex_net/lib/python3.9/site-packages/torch/library.py:184\u001b[0m, in \u001b[0;36m_register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    181\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_ops\u001b[38;5;241m.\u001b[39m_refresh_packet(packet)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_defs\u001b[38;5;241m.\u001b[39madd(qualname)\n\u001b[0;32m--> 184\u001b[0m _defs\u001b[38;5;241m.\u001b[39madd(qualname)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/complex_net/lib/python3.9/site-packages/torch/_library/fake_impl.py:31\u001b[0m, in \u001b[0;36mFakeImplHolder.register\u001b[0;34m(self, func, source)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an fake impl registered at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_has_kernel_for_dispatch_key(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeImplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import librosa\n",
    "from scipy import signal as sci_signal\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import efficientnet\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from datetime import datetime  \n",
    "\n",
    "# # import score function of BirdCLEF\n",
    "# sys.path.append('/kaggle/input/birdclef-roc-auc')\n",
    "# sys.path.append('/kaggle/usr/lib/kaggle_metric_utilities')\n",
    "# from metric import score\n",
    "# print('Imports done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Hyper-paramters \n",
    "\n",
    "# Note batch diminué pour résoudre out of memory cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:50:25.723817Z",
     "iopub.status.busy": "2025-07-03T18:50:25.723165Z",
     "iopub.status.idle": "2025-07-03T18:50:25.732180Z",
     "shell.execute_reply": "2025-07-03T18:50:25.731073Z",
     "shell.execute_reply.started": "2025-07-03T18:50:25.723785Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed:\n",
      "2024\n",
      "2025-07-04 09:18 : Config imported\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    \n",
    "    # == global config ==\n",
    "    SEED = 2024  # random seed\n",
    "    DEVICE = 'cuda'  # device to be used\n",
    "    MIXED_PRECISION = False  # whether to use mixed-16 precision\n",
    "    OUTPUT_DIR = '/kaggle/working/'  # output folder\n",
    "    \n",
    "    # == data config ==\n",
    "    DATA_ROOT = '/home/christophe/birdclef'  # root folder\n",
    "    PREPROCESSED_DATA_ROOT = '/kaggle/input/birdclef24-spectrograms-via-cupy'\n",
    "    LOAD_DATA = True  # whether to load data from pre-processed dataset\n",
    "    FS = 32000  # sample rate\n",
    "    N_FFT = 1095  # n FFT of Spec.\n",
    "    WIN_SIZE = 412  # WIN_SIZE of Spec.\n",
    "    WIN_LAP = 100  # overlap of Spec.\n",
    "    MIN_FREQ = 40  # min frequency\n",
    "    MAX_FREQ = 15000  # max frequency\n",
    "    \n",
    "    # == model config ==\n",
    "    MODEL_TYPE = 'efficientnet_b0'  # model type\n",
    "    \n",
    "    # == dataset config ==\n",
    "    BATCH_SIZE = 4  # batch size of each step\n",
    "    N_WORKERS = 4  # number of workers\n",
    "    \n",
    "    # == AUG ==\n",
    "    USE_XYMASKING = True  # whether use XYMasking\n",
    "    \n",
    "    # == training config ==\n",
    "    FOLDS = 10  # n fold\n",
    "    EPOCHS = 2  # max epochs initialy 15\n",
    "    LR = 1e-3  # learning rate\n",
    "    WEIGHT_DECAY = 1e-5  # weight decay of optimizer\n",
    "    \n",
    "    # == other config ==\n",
    "    VISUALIZE = True  # whether to visualize data and batch\n",
    "\n",
    "\n",
    "\n",
    "def print_date_with_message(message):\n",
    "    now = datetime.now()\n",
    "    formatted_date_time = now.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    print(f\"{formatted_date_time} : {message}\")\n",
    "\n",
    "print('fix seed:')\n",
    "print(config.SEED)\n",
    "pl.seed_everything(config.SEED, workers=True)\n",
    "\n",
    "print_date_with_message(\"Config imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:50:38.893764Z",
     "iopub.status.busy": "2025-07-03T18:50:38.893095Z",
     "iopub.status.idle": "2025-07-03T18:50:38.898627Z",
     "shell.execute_reply": "2025-07-03T18:50:38.897647Z",
     "shell.execute_reply.started": "2025-07-03T18:50:38.893733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# labels\n",
    "label_list = sorted(os.listdir(os.path.join(config.DATA_ROOT, 'train_audio')))\n",
    "label_id_list = list(range(len(label_list)))\n",
    "label2id = dict(zip(label_list, label_id_list))\n",
    "id2label = dict(zip(label_id_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asbfly': 0,\n",
       " 'ashdro1': 1,\n",
       " 'ashpri1': 2,\n",
       " 'ashwoo2': 3,\n",
       " 'asikoe2': 4,\n",
       " 'asiope1': 5,\n",
       " 'aspfly1': 6,\n",
       " 'aspswi1': 7,\n",
       " 'barfly1': 8,\n",
       " 'barswa': 9,\n",
       " 'bcnher': 10,\n",
       " 'bkcbul1': 11,\n",
       " 'bkrfla1': 12,\n",
       " 'bkskit1': 13,\n",
       " 'bkwsti': 14,\n",
       " 'bladro1': 15,\n",
       " 'blaeag1': 16,\n",
       " 'blakit1': 17,\n",
       " 'blhori1': 18,\n",
       " 'blnmon1': 19,\n",
       " 'blrwar1': 20,\n",
       " 'bncwoo3': 21,\n",
       " 'brakit1': 22,\n",
       " 'brasta1': 23,\n",
       " 'brcful1': 24,\n",
       " 'brfowl1': 25,\n",
       " 'brnhao1': 26,\n",
       " 'brnshr': 27,\n",
       " 'brodro1': 28,\n",
       " 'brwjac1': 29,\n",
       " 'brwowl1': 30,\n",
       " 'btbeat1': 31,\n",
       " 'bwfshr1': 32,\n",
       " 'categr': 33,\n",
       " 'chbeat1': 34,\n",
       " 'cohcuc1': 35,\n",
       " 'comfla1': 36,\n",
       " 'comgre': 37,\n",
       " 'comior1': 38,\n",
       " 'comkin1': 39,\n",
       " 'commoo3': 40,\n",
       " 'commyn': 41,\n",
       " 'compea': 42,\n",
       " 'comros': 43,\n",
       " 'comsan': 44,\n",
       " 'comtai1': 45,\n",
       " 'copbar1': 46,\n",
       " 'crbsun2': 47,\n",
       " 'cregos1': 48,\n",
       " 'crfbar1': 49,\n",
       " 'crseag1': 50,\n",
       " 'dafbab1': 51,\n",
       " 'darter2': 52,\n",
       " 'eaywag1': 53,\n",
       " 'emedov2': 54,\n",
       " 'eucdov': 55,\n",
       " 'eurbla2': 56,\n",
       " 'eurcoo': 57,\n",
       " 'forwag1': 58,\n",
       " 'gargan': 59,\n",
       " 'gloibi': 60,\n",
       " 'goflea1': 61,\n",
       " 'graher1': 62,\n",
       " 'grbeat1': 63,\n",
       " 'grecou1': 64,\n",
       " 'greegr': 65,\n",
       " 'grefla1': 66,\n",
       " 'grehor1': 67,\n",
       " 'grejun2': 68,\n",
       " 'grenig1': 69,\n",
       " 'grewar3': 70,\n",
       " 'grnsan': 71,\n",
       " 'grnwar1': 72,\n",
       " 'grtdro1': 73,\n",
       " 'gryfra': 74,\n",
       " 'grynig2': 75,\n",
       " 'grywag': 76,\n",
       " 'gybpri1': 77,\n",
       " 'gyhcaf1': 78,\n",
       " 'heswoo1': 79,\n",
       " 'hoopoe': 80,\n",
       " 'houcro1': 81,\n",
       " 'houspa': 82,\n",
       " 'inbrob1': 83,\n",
       " 'indpit1': 84,\n",
       " 'indrob1': 85,\n",
       " 'indrol2': 86,\n",
       " 'indtit1': 87,\n",
       " 'ingori1': 88,\n",
       " 'inpher1': 89,\n",
       " 'insbab1': 90,\n",
       " 'insowl1': 91,\n",
       " 'integr': 92,\n",
       " 'isbduc1': 93,\n",
       " 'jerbus2': 94,\n",
       " 'junbab2': 95,\n",
       " 'junmyn1': 96,\n",
       " 'junowl1': 97,\n",
       " 'kenplo1': 98,\n",
       " 'kerlau2': 99,\n",
       " 'labcro1': 100,\n",
       " 'laudov1': 101,\n",
       " 'lblwar1': 102,\n",
       " 'lesyel1': 103,\n",
       " 'lewduc1': 104,\n",
       " 'lirplo': 105,\n",
       " 'litegr': 106,\n",
       " 'litgre1': 107,\n",
       " 'litspi1': 108,\n",
       " 'litswi1': 109,\n",
       " 'lobsun2': 110,\n",
       " 'maghor2': 111,\n",
       " 'malpar1': 112,\n",
       " 'maltro1': 113,\n",
       " 'malwoo1': 114,\n",
       " 'marsan': 115,\n",
       " 'mawthr1': 116,\n",
       " 'moipig1': 117,\n",
       " 'nilfly2': 118,\n",
       " 'niwpig1': 119,\n",
       " 'nutman': 120,\n",
       " 'orihob2': 121,\n",
       " 'oripip1': 122,\n",
       " 'pabflo1': 123,\n",
       " 'paisto1': 124,\n",
       " 'piebus1': 125,\n",
       " 'piekin1': 126,\n",
       " 'placuc3': 127,\n",
       " 'plaflo1': 128,\n",
       " 'plapri1': 129,\n",
       " 'plhpar1': 130,\n",
       " 'pomgrp2': 131,\n",
       " 'purher1': 132,\n",
       " 'pursun3': 133,\n",
       " 'pursun4': 134,\n",
       " 'purswa3': 135,\n",
       " 'putbab1': 136,\n",
       " 'redspu1': 137,\n",
       " 'rerswa1': 138,\n",
       " 'revbul': 139,\n",
       " 'rewbul': 140,\n",
       " 'rewlap1': 141,\n",
       " 'rocpig': 142,\n",
       " 'rorpar': 143,\n",
       " 'rossta2': 144,\n",
       " 'rufbab3': 145,\n",
       " 'ruftre2': 146,\n",
       " 'rufwoo2': 147,\n",
       " 'rutfly6': 148,\n",
       " 'sbeowl1': 149,\n",
       " 'scamin3': 150,\n",
       " 'shikra1': 151,\n",
       " 'smamin1': 152,\n",
       " 'sohmyn1': 153,\n",
       " 'spepic1': 154,\n",
       " 'spodov': 155,\n",
       " 'spoowl1': 156,\n",
       " 'sqtbul1': 157,\n",
       " 'stbkin1': 158,\n",
       " 'sttwoo1': 159,\n",
       " 'thbwar1': 160,\n",
       " 'tibfly3': 161,\n",
       " 'tilwar1': 162,\n",
       " 'vefnut1': 163,\n",
       " 'vehpar1': 164,\n",
       " 'wbbfly1': 165,\n",
       " 'wemhar1': 166,\n",
       " 'whbbul2': 167,\n",
       " 'whbsho3': 168,\n",
       " 'whbtre1': 169,\n",
       " 'whbwag1': 170,\n",
       " 'whbwat1': 171,\n",
       " 'whbwoo2': 172,\n",
       " 'whcbar1': 173,\n",
       " 'whiter2': 174,\n",
       " 'whrmun': 175,\n",
       " 'whtkin2': 176,\n",
       " 'woosan': 177,\n",
       " 'wynlau1': 178,\n",
       " 'yebbab1': 179,\n",
       " 'yebbul3': 180,\n",
       " 'zitcis1': 181}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'asbfly',\n",
       " 1: 'ashdro1',\n",
       " 2: 'ashpri1',\n",
       " 3: 'ashwoo2',\n",
       " 4: 'asikoe2',\n",
       " 5: 'asiope1',\n",
       " 6: 'aspfly1',\n",
       " 7: 'aspswi1',\n",
       " 8: 'barfly1',\n",
       " 9: 'barswa',\n",
       " 10: 'bcnher',\n",
       " 11: 'bkcbul1',\n",
       " 12: 'bkrfla1',\n",
       " 13: 'bkskit1',\n",
       " 14: 'bkwsti',\n",
       " 15: 'bladro1',\n",
       " 16: 'blaeag1',\n",
       " 17: 'blakit1',\n",
       " 18: 'blhori1',\n",
       " 19: 'blnmon1',\n",
       " 20: 'blrwar1',\n",
       " 21: 'bncwoo3',\n",
       " 22: 'brakit1',\n",
       " 23: 'brasta1',\n",
       " 24: 'brcful1',\n",
       " 25: 'brfowl1',\n",
       " 26: 'brnhao1',\n",
       " 27: 'brnshr',\n",
       " 28: 'brodro1',\n",
       " 29: 'brwjac1',\n",
       " 30: 'brwowl1',\n",
       " 31: 'btbeat1',\n",
       " 32: 'bwfshr1',\n",
       " 33: 'categr',\n",
       " 34: 'chbeat1',\n",
       " 35: 'cohcuc1',\n",
       " 36: 'comfla1',\n",
       " 37: 'comgre',\n",
       " 38: 'comior1',\n",
       " 39: 'comkin1',\n",
       " 40: 'commoo3',\n",
       " 41: 'commyn',\n",
       " 42: 'compea',\n",
       " 43: 'comros',\n",
       " 44: 'comsan',\n",
       " 45: 'comtai1',\n",
       " 46: 'copbar1',\n",
       " 47: 'crbsun2',\n",
       " 48: 'cregos1',\n",
       " 49: 'crfbar1',\n",
       " 50: 'crseag1',\n",
       " 51: 'dafbab1',\n",
       " 52: 'darter2',\n",
       " 53: 'eaywag1',\n",
       " 54: 'emedov2',\n",
       " 55: 'eucdov',\n",
       " 56: 'eurbla2',\n",
       " 57: 'eurcoo',\n",
       " 58: 'forwag1',\n",
       " 59: 'gargan',\n",
       " 60: 'gloibi',\n",
       " 61: 'goflea1',\n",
       " 62: 'graher1',\n",
       " 63: 'grbeat1',\n",
       " 64: 'grecou1',\n",
       " 65: 'greegr',\n",
       " 66: 'grefla1',\n",
       " 67: 'grehor1',\n",
       " 68: 'grejun2',\n",
       " 69: 'grenig1',\n",
       " 70: 'grewar3',\n",
       " 71: 'grnsan',\n",
       " 72: 'grnwar1',\n",
       " 73: 'grtdro1',\n",
       " 74: 'gryfra',\n",
       " 75: 'grynig2',\n",
       " 76: 'grywag',\n",
       " 77: 'gybpri1',\n",
       " 78: 'gyhcaf1',\n",
       " 79: 'heswoo1',\n",
       " 80: 'hoopoe',\n",
       " 81: 'houcro1',\n",
       " 82: 'houspa',\n",
       " 83: 'inbrob1',\n",
       " 84: 'indpit1',\n",
       " 85: 'indrob1',\n",
       " 86: 'indrol2',\n",
       " 87: 'indtit1',\n",
       " 88: 'ingori1',\n",
       " 89: 'inpher1',\n",
       " 90: 'insbab1',\n",
       " 91: 'insowl1',\n",
       " 92: 'integr',\n",
       " 93: 'isbduc1',\n",
       " 94: 'jerbus2',\n",
       " 95: 'junbab2',\n",
       " 96: 'junmyn1',\n",
       " 97: 'junowl1',\n",
       " 98: 'kenplo1',\n",
       " 99: 'kerlau2',\n",
       " 100: 'labcro1',\n",
       " 101: 'laudov1',\n",
       " 102: 'lblwar1',\n",
       " 103: 'lesyel1',\n",
       " 104: 'lewduc1',\n",
       " 105: 'lirplo',\n",
       " 106: 'litegr',\n",
       " 107: 'litgre1',\n",
       " 108: 'litspi1',\n",
       " 109: 'litswi1',\n",
       " 110: 'lobsun2',\n",
       " 111: 'maghor2',\n",
       " 112: 'malpar1',\n",
       " 113: 'maltro1',\n",
       " 114: 'malwoo1',\n",
       " 115: 'marsan',\n",
       " 116: 'mawthr1',\n",
       " 117: 'moipig1',\n",
       " 118: 'nilfly2',\n",
       " 119: 'niwpig1',\n",
       " 120: 'nutman',\n",
       " 121: 'orihob2',\n",
       " 122: 'oripip1',\n",
       " 123: 'pabflo1',\n",
       " 124: 'paisto1',\n",
       " 125: 'piebus1',\n",
       " 126: 'piekin1',\n",
       " 127: 'placuc3',\n",
       " 128: 'plaflo1',\n",
       " 129: 'plapri1',\n",
       " 130: 'plhpar1',\n",
       " 131: 'pomgrp2',\n",
       " 132: 'purher1',\n",
       " 133: 'pursun3',\n",
       " 134: 'pursun4',\n",
       " 135: 'purswa3',\n",
       " 136: 'putbab1',\n",
       " 137: 'redspu1',\n",
       " 138: 'rerswa1',\n",
       " 139: 'revbul',\n",
       " 140: 'rewbul',\n",
       " 141: 'rewlap1',\n",
       " 142: 'rocpig',\n",
       " 143: 'rorpar',\n",
       " 144: 'rossta2',\n",
       " 145: 'rufbab3',\n",
       " 146: 'ruftre2',\n",
       " 147: 'rufwoo2',\n",
       " 148: 'rutfly6',\n",
       " 149: 'sbeowl1',\n",
       " 150: 'scamin3',\n",
       " 151: 'shikra1',\n",
       " 152: 'smamin1',\n",
       " 153: 'sohmyn1',\n",
       " 154: 'spepic1',\n",
       " 155: 'spodov',\n",
       " 156: 'spoowl1',\n",
       " 157: 'sqtbul1',\n",
       " 158: 'stbkin1',\n",
       " 159: 'sttwoo1',\n",
       " 160: 'thbwar1',\n",
       " 161: 'tibfly3',\n",
       " 162: 'tilwar1',\n",
       " 163: 'vefnut1',\n",
       " 164: 'vehpar1',\n",
       " 165: 'wbbfly1',\n",
       " 166: 'wemhar1',\n",
       " 167: 'whbbul2',\n",
       " 168: 'whbsho3',\n",
       " 169: 'whbtre1',\n",
       " 170: 'whbwag1',\n",
       " 171: 'whbwat1',\n",
       " 172: 'whbwoo2',\n",
       " 173: 'whcbar1',\n",
       " 174: 'whiter2',\n",
       " 175: 'whrmun',\n",
       " 176: 'whtkin2',\n",
       " 177: 'woosan',\n",
       " 178: 'wynlau1',\n",
       " 179: 'yebbab1',\n",
       " 180: 'yebbul3',\n",
       " 181: 'zitcis1'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:50:51.089430Z",
     "iopub.status.busy": "2025-07-03T18:50:51.088733Z",
     "iopub.status.idle": "2025-07-03T18:50:51.094625Z",
     "shell.execute_reply": "2025-07-03T18:50:51.093579Z",
     "shell.execute_reply.started": "2025-07-03T18:50:51.089401Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Using 1 GPU(s)\n",
      "2.5.1\n",
      "True\n",
      "11.8\n",
      "/home/christophe/miniforge3/envs/complex_net/lib/python3.9/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "#print('for now we will use only one gpu')\n",
    "import os\n",
    "import torch\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print('Using', torch.cuda.device_count(), 'GPU(s)')\n",
    "print(torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Check CUDA version\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Check the location of PyTorch installation\n",
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader\n",
    "\n",
    "1. [Load Metadata](#Load-Metadata): Load metadata from dataset\n",
    "2. [Pre-Processing](#Pre-Processing): The function to convert audio to spectrograms.\n",
    "3. [Dataset](#Dataset): Yield samples\n",
    "4. [Augmentation](#Augmentation): Data augmentation\n",
    "5. [Verify](#Verify): Verify the dataset and dataloader work well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:50:55.441114Z",
     "iopub.status.busy": "2025-07-03T18:50:55.440811Z",
     "iopub.status.idle": "2025-07-03T18:50:55.573629Z",
     "shell.execute_reply": "2025-07-03T18:50:55.572568Z",
     "shell.execute_reply.started": "2025-07-03T18:50:55.441092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>39.2297</td>\n",
       "      <td>118.1987</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Matt Slaymaker</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.xeno-canto.org/134896</td>\n",
       "      <td>asbfly/XC134896.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song']</td>\n",
       "      <td>51.4030</td>\n",
       "      <td>104.6401</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Magnus Hellström</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>https://www.xeno-canto.org/164848</td>\n",
       "      <td>asbfly/XC164848.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song']</td>\n",
       "      <td>36.3319</td>\n",
       "      <td>127.3555</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Stuart Fisher</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>https://www.xeno-canto.org/175797</td>\n",
       "      <td>asbfly/XC175797.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>21.1697</td>\n",
       "      <td>70.6005</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>vir joshi</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.xeno-canto.org/207738</td>\n",
       "      <td>asbfly/XC207738.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>15.5442</td>\n",
       "      <td>73.7733</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Albert Lastukhin &amp; Sergei Karpeev</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.xeno-canto.org/209218</td>\n",
       "      <td>asbfly/XC209218.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label secondary_labels      type  latitude  longitude  \\\n",
       "0        asbfly               []  ['call']   39.2297   118.1987   \n",
       "1        asbfly               []  ['song']   51.4030   104.6401   \n",
       "2        asbfly               []  ['song']   36.3319   127.3555   \n",
       "3        asbfly               []  ['call']   21.1697    70.6005   \n",
       "4        asbfly               []  ['call']   15.5442    73.7733   \n",
       "\n",
       "      scientific_name             common_name  \\\n",
       "0  Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "1  Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "2  Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "3  Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "4  Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "\n",
       "                              author  \\\n",
       "0                     Matt Slaymaker   \n",
       "1                   Magnus Hellström   \n",
       "2                      Stuart Fisher   \n",
       "3                          vir joshi   \n",
       "4  Albert Lastukhin & Sergei Karpeev   \n",
       "\n",
       "                                             license  rating  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     5.0   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     2.5   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     2.5   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     4.0   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     4.0   \n",
       "\n",
       "                                 url             filename  \n",
       "0  https://www.xeno-canto.org/134896  asbfly/XC134896.ogg  \n",
       "1  https://www.xeno-canto.org/164848  asbfly/XC164848.ogg  \n",
       "2  https://www.xeno-canto.org/175797  asbfly/XC175797.ogg  \n",
       "3  https://www.xeno-canto.org/207738  asbfly/XC207738.ogg  \n",
       "4  https://www.xeno-canto.org/209218  asbfly/XC209218.ogg  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(f'{config.DATA_ROOT}/train_metadata.csv')\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:25:20.826643Z",
     "iopub.status.busy": "2025-07-03T18:25:20.825983Z",
     "iopub.status.idle": "2025-07-03T18:25:20.831617Z",
     "shell.execute_reply": "2025-07-03T18:25:20.830553Z",
     "shell.execute_reply.started": "2025-07-03T18:25:20.826613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary_label\n",
      "secondary_labels\n",
      "type\n",
      "latitude\n",
      "longitude\n",
      "scientific_name\n",
      "common_name\n",
      "author\n",
      "license\n",
      "rating\n",
      "url\n",
      "filename\n"
     ]
    }
   ],
   "source": [
    "for col in metadata_df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column target assigns a class to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous créons notre dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:51:01.385434Z",
     "iopub.status.busy": "2025-07-03T18:51:01.384405Z",
     "iopub.status.idle": "2025-07-03T18:51:01.432589Z",
     "shell.execute_reply": "2025-07-03T18:51:01.431637Z",
     "shell.execute_reply.started": "2025-07-03T18:51:01.385391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 24459 samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>rating</th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>filepath</th>\n",
       "      <th>samplename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>brodro1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>brodro1/XC843971.ogg</td>\n",
       "      <td>28</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/brodro...</td>\n",
       "      <td>brodro1-XC843971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9055</th>\n",
       "      <td>eucdov</td>\n",
       "      <td>4.5</td>\n",
       "      <td>eucdov/XC128849.ogg</td>\n",
       "      <td>55</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/eucdov...</td>\n",
       "      <td>eucdov-XC128849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>comkin1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>comkin1/XC742886.ogg</td>\n",
       "      <td>39</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/comkin...</td>\n",
       "      <td>comkin1-XC742886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15966</th>\n",
       "      <td>labcro1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>labcro1/XC143395.ogg</td>\n",
       "      <td>100</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/labcro...</td>\n",
       "      <td>labcro1-XC143395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20837</th>\n",
       "      <td>rorpar</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rorpar/XC505788.ogg</td>\n",
       "      <td>143</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/rorpar...</td>\n",
       "      <td>rorpar-XC505788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5225</th>\n",
       "      <td>comgre</td>\n",
       "      <td>4.0</td>\n",
       "      <td>comgre/XC820709.ogg</td>\n",
       "      <td>37</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/comgre...</td>\n",
       "      <td>comgre-XC820709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8578</th>\n",
       "      <td>eaywag1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>eaywag1/XC338351.ogg</td>\n",
       "      <td>53</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/eaywag...</td>\n",
       "      <td>eaywag1-XC338351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17159</th>\n",
       "      <td>litegr</td>\n",
       "      <td>3.0</td>\n",
       "      <td>litegr/XC589946.ogg</td>\n",
       "      <td>106</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/litegr...</td>\n",
       "      <td>litegr-XC589946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>bcnher</td>\n",
       "      <td>5.0</td>\n",
       "      <td>bcnher/XC742880.ogg</td>\n",
       "      <td>10</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/bcnher...</td>\n",
       "      <td>bcnher-XC742880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8082</th>\n",
       "      <td>comtai1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>comtai1/XC788137.ogg</td>\n",
       "      <td>45</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/comtai...</td>\n",
       "      <td>comtai1-XC788137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18587</th>\n",
       "      <td>pabflo1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>pabflo1/XC741595.ogg</td>\n",
       "      <td>123</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/pabflo...</td>\n",
       "      <td>pabflo1-XC741595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8850</th>\n",
       "      <td>eaywag1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>eaywag1/XC677591.ogg</td>\n",
       "      <td>53</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/eaywag...</td>\n",
       "      <td>eaywag1-XC677591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15149</th>\n",
       "      <td>inbrob1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>inbrob1/XC402423.ogg</td>\n",
       "      <td>83</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/inbrob...</td>\n",
       "      <td>inbrob1-XC402423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4077</th>\n",
       "      <td>brnshr</td>\n",
       "      <td>3.0</td>\n",
       "      <td>brnshr/XC469463.ogg</td>\n",
       "      <td>27</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/brnshr...</td>\n",
       "      <td>brnshr-XC469463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>bkskit1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>bkskit1/XC170108.ogg</td>\n",
       "      <td>13</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/bkskit...</td>\n",
       "      <td>bkskit1-XC170108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22528</th>\n",
       "      <td>whbbul2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>whbbul2/XC453725.ogg</td>\n",
       "      <td>167</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/whbbul...</td>\n",
       "      <td>whbbul2-XC453725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>bkwsti</td>\n",
       "      <td>5.0</td>\n",
       "      <td>bkwsti/XC632959.ogg</td>\n",
       "      <td>14</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/bkwsti...</td>\n",
       "      <td>bkwsti-XC632959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10224</th>\n",
       "      <td>gargan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>gargan/XC711488.ogg</td>\n",
       "      <td>59</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/gargan...</td>\n",
       "      <td>gargan-XC711488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>comgre</td>\n",
       "      <td>5.0</td>\n",
       "      <td>comgre/XC738227.ogg</td>\n",
       "      <td>37</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/comgre...</td>\n",
       "      <td>comgre-XC738227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8520</th>\n",
       "      <td>eaywag1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>eaywag1/XC175696.ogg</td>\n",
       "      <td>53</td>\n",
       "      <td>/kaggle/input/birdclef-2024/train_audio/eaywag...</td>\n",
       "      <td>eaywag1-XC175696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      primary_label  rating              filename  target  \\\n",
       "4253        brodro1     4.0  brodro1/XC843971.ogg      28   \n",
       "9055         eucdov     4.5   eucdov/XC128849.ogg      55   \n",
       "5924        comkin1     4.0  comkin1/XC742886.ogg      39   \n",
       "15966       labcro1     4.0  labcro1/XC143395.ogg     100   \n",
       "20837        rorpar     1.0   rorpar/XC505788.ogg     143   \n",
       "5225         comgre     4.0   comgre/XC820709.ogg      37   \n",
       "8578        eaywag1     4.0  eaywag1/XC338351.ogg      53   \n",
       "17159        litegr     3.0   litegr/XC589946.ogg     106   \n",
       "1583         bcnher     5.0   bcnher/XC742880.ogg      10   \n",
       "8082        comtai1     5.0  comtai1/XC788137.ogg      45   \n",
       "18587       pabflo1     5.0  pabflo1/XC741595.ogg     123   \n",
       "8850        eaywag1     4.5  eaywag1/XC677591.ogg      53   \n",
       "15149       inbrob1     3.5  inbrob1/XC402423.ogg      83   \n",
       "4077         brnshr     3.0   brnshr/XC469463.ogg      27   \n",
       "1761        bkskit1     5.0  bkskit1/XC170108.ogg      13   \n",
       "22528       whbbul2     5.0  whbbul2/XC453725.ogg     167   \n",
       "2306         bkwsti     5.0   bkwsti/XC632959.ogg      14   \n",
       "10224        gargan     4.0   gargan/XC711488.ogg      59   \n",
       "5139         comgre     5.0   comgre/XC738227.ogg      37   \n",
       "8520        eaywag1     2.5  eaywag1/XC175696.ogg      53   \n",
       "\n",
       "                                                filepath        samplename  \n",
       "4253   /kaggle/input/birdclef-2024/train_audio/brodro...  brodro1-XC843971  \n",
       "9055   /kaggle/input/birdclef-2024/train_audio/eucdov...   eucdov-XC128849  \n",
       "5924   /kaggle/input/birdclef-2024/train_audio/comkin...  comkin1-XC742886  \n",
       "15966  /kaggle/input/birdclef-2024/train_audio/labcro...  labcro1-XC143395  \n",
       "20837  /kaggle/input/birdclef-2024/train_audio/rorpar...   rorpar-XC505788  \n",
       "5225   /kaggle/input/birdclef-2024/train_audio/comgre...   comgre-XC820709  \n",
       "8578   /kaggle/input/birdclef-2024/train_audio/eaywag...  eaywag1-XC338351  \n",
       "17159  /kaggle/input/birdclef-2024/train_audio/litegr...   litegr-XC589946  \n",
       "1583   /kaggle/input/birdclef-2024/train_audio/bcnher...   bcnher-XC742880  \n",
       "8082   /kaggle/input/birdclef-2024/train_audio/comtai...  comtai1-XC788137  \n",
       "18587  /kaggle/input/birdclef-2024/train_audio/pabflo...  pabflo1-XC741595  \n",
       "8850   /kaggle/input/birdclef-2024/train_audio/eaywag...  eaywag1-XC677591  \n",
       "15149  /kaggle/input/birdclef-2024/train_audio/inbrob...  inbrob1-XC402423  \n",
       "4077   /kaggle/input/birdclef-2024/train_audio/brnshr...   brnshr-XC469463  \n",
       "1761   /kaggle/input/birdclef-2024/train_audio/bkskit...  bkskit1-XC170108  \n",
       "22528  /kaggle/input/birdclef-2024/train_audio/whbbul...  whbbul2-XC453725  \n",
       "2306   /kaggle/input/birdclef-2024/train_audio/bkwsti...   bkwsti-XC632959  \n",
       "10224  /kaggle/input/birdclef-2024/train_audio/gargan...   gargan-XC711488  \n",
       "5139   /kaggle/input/birdclef-2024/train_audio/comgre...   comgre-XC738227  \n",
       "8520   /kaggle/input/birdclef-2024/train_audio/eaywag...  eaywag1-XC175696  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = metadata_df[['primary_label', 'rating', 'filename']].copy()\n",
    "\n",
    "# create target\n",
    "train_df['target'] = train_df.primary_label.map(label2id) # assigns an int to bird species\n",
    "# create filepath\n",
    "train_df['filepath'] = config.DATA_ROOT + '/train_audio/' + train_df.filename\n",
    "# create new sample name\n",
    "train_df['samplename'] = train_df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "\n",
    "print(f'found {len(train_df)} samples')\n",
    "\n",
    "train_df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "To speed up audio-to-spectrogram, we employ CuPy. `CuPy is a NumPy/SciPy-compatible array library for GPU-accelerated computing with Python,` which can significant improve the efficiency of conversion. For more detailed analysis, you can refer to this [notebook](https://www.kaggle.com/code/zijiangyang1116/birdclef-24-speed-up-audio-to-spec-via-cupy).\n",
    "\n",
    "Please note, in this notebook, we only use the **center 5 sec** of each audio. By default (`Load_DATA=True`), pre-processed data will be loaded from the [dataset](https://www.kaggle.com/datasets/zijiangyang1116/birdclef24-spectrograms-via-cupy). If `Load_DATA` is set to `False`, spectrograms will be create with `CuPy` (about 30 minites)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous n'utilserons pas ces fonctions puisque notre réseau s'en charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:51:06.292476Z",
     "iopub.status.busy": "2025-07-03T18:51:06.291592Z",
     "iopub.status.idle": "2025-07-03T18:51:06.298612Z",
     "shell.execute_reply": "2025-07-03T18:51:06.297660Z",
     "shell.execute_reply.started": "2025-07-03T18:51:06.292445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def oog2spec_via_scipy(audio_data):\n",
    "    # handles NaNs\n",
    "    mean_signal = np.nanmean(audio_data)\n",
    "    audio_data = np.nan_to_num(audio_data, nan=mean_signal) if np.isnan(audio_data).mean() < 1 else np.zeros_like(audio_data)\n",
    "    \n",
    "    # to spec.\n",
    "    frequencies, times, spec_data = sci_signal.spectrogram(\n",
    "        audio_data, \n",
    "        fs=config.FS, \n",
    "        nfft=config.N_FFT, \n",
    "        nperseg=config.WIN_SIZE, \n",
    "        noverlap=config.WIN_LAP, \n",
    "        window='hann'\n",
    "    )\n",
    "    \n",
    "    # Filter frequency range\n",
    "    valid_freq = (frequencies >= config.MIN_FREQ) & (frequencies <= config.MAX_FREQ)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # Log\n",
    "    spec_data = np.log10(spec_data + 1e-20)\n",
    "    \n",
    "    # min/max normalize\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "    \n",
    "    return spec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:51:09.931829Z",
     "iopub.status.busy": "2025-07-03T18:51:09.931466Z",
     "iopub.status.idle": "2025-07-03T18:51:09.938841Z",
     "shell.execute_reply": "2025-07-03T18:51:09.937875Z",
     "shell.execute_reply.started": "2025-07-03T18:51:09.931800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def oog2spec_via_cupy(audio_data):\n",
    "    \n",
    "    import cupy as cp\n",
    "    from cupyx.scipy import signal as cupy_signal\n",
    "    \n",
    "    audio_data = cp.array(audio_data)\n",
    "    \n",
    "    # handles NaNs\n",
    "    mean_signal = cp.nanmean(audio_data)\n",
    "    audio_data = cp.nan_to_num(audio_data, nan=mean_signal) if cp.isnan(audio_data).mean() < 1 else cp.zeros_like(audio_data)\n",
    "    \n",
    "    # to spec.\n",
    "    frequencies, times, spec_data = cupy_signal.spectrogram(\n",
    "        audio_data, \n",
    "        fs=config.FS, \n",
    "        nfft=config.N_FFT, \n",
    "        nperseg=config.WIN_SIZE, \n",
    "        noverlap=config.WIN_LAP, \n",
    "        window='hann'\n",
    "    )\n",
    "    \n",
    "    # Filter frequency range\n",
    "    valid_freq = (frequencies >= config.MIN_FREQ) & (frequencies <= config.MAX_FREQ)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # Log\n",
    "    spec_data = cp.log10(spec_data + 1e-20)\n",
    "    \n",
    "    # min/max normalize\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "    \n",
    "    return spec_data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T19:19:25.828606Z",
     "iopub.status.busy": "2025-07-02T19:19:25.828238Z",
     "iopub.status.idle": "2025-07-02T19:19:25.837448Z",
     "shell.execute_reply": "2025-07-02T19:19:25.836364Z",
     "shell.execute_reply.started": "2025-07-02T19:19:25.828581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "This code must not be executed",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m This code must not be executed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.exit('This code must not be executed')\n",
    "if config.LOAD_DATA:\n",
    "    print('load from file')\n",
    "    all_bird_data = np.load(f'{config.PREPROCESSED_DATA_ROOT}/spec_center_5sec_256_256.npy', allow_pickle=True).item()\n",
    "else:\n",
    "    all_bird_data = dict()\n",
    "    for i, row_metadata in tqdm(train_df.iterrows()):\n",
    "\n",
    "        # load ogg\n",
    "        audio_data, _ = librosa.load(row_metadata.filepath, sr=config.FS)\n",
    "\n",
    "        # crop\n",
    "        n_copy = math.ceil(5 * config.FS / len(audio_data))\n",
    "        if n_copy > 1: audio_data = np.concatenate([audio_data]*n_copy)\n",
    "\n",
    "        start_idx = int(len(audio_data) / 2 - 2.5 * config.FS)\n",
    "        end_idx = int(start_idx + 5.0 * config.FS)\n",
    "        input_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "        # ogg to spec.\n",
    "        input_spec = oog2spec_via_cupy(input_audio)\n",
    "        \n",
    "        input_spec = cv2.resize(input_spec, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        all_bird_data[row_metadata.samplename] = input_spec.astype(np.float32)\n",
    "\n",
    "    # save to file\n",
    "    np.save(os.path.join(config.OUTPUT_DIR, f'spec_center_5sec_256_256.npy'), all_bird_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To yield samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nous pourrions ajouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:51:21.586551Z",
     "iopub.status.busy": "2025-07-03T18:51:21.585890Z",
     "iopub.status.idle": "2025-07-03T18:51:21.596105Z",
     "shell.execute_reply": "2025-07-03T18:51:21.595099Z",
     "shell.execute_reply.started": "2025-07-03T18:51:21.586521Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-03 - dataset compiled\n"
     ]
    }
   ],
   "source": [
    "#from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "# not used fo now\n",
    "def augment_audio(audio):\n",
    "    # Random gain\n",
    "    gain = np.random.uniform(0.8, 1.2)\n",
    "    audio *= gain\n",
    "\n",
    "    # Add random noise\n",
    "    noise = np.random.normal(0, 0.005, audio.shape)\n",
    "    audio += noise\n",
    "\n",
    "    return audio\n",
    "\n",
    "class BirdDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata,\n",
    "        augmentation=None,\n",
    "        mode='train'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        #self.augmentation = augmentation\n",
    "        self.mode = mode\n",
    "       # self.augmentation = Compose([\n",
    "            #AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "          #  TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "          #  PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "            #Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
    "        #])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row_metadata = self.metadata.iloc[index]\n",
    "\n",
    "        audio_data, _ = librosa.load(row_metadata.filepath, sr=config.FS)\n",
    "\n",
    "        n_copy = math.ceil(5 * config.FS / len(audio_data))\n",
    "        if n_copy > 1: audio_data = np.concatenate([audio_data]*n_copy)\n",
    "\n",
    "        start_idx = int(len(audio_data) / 2 - 2.5 * config.FS)\n",
    "        end_idx = int(start_idx + 5.0 * config.FS)\n",
    "        input_audio = audio_data[start_idx:end_idx]\n",
    "        # for now no data augmentatio in train mode\n",
    "        #if self.augmentation and self.mode=='aug':\n",
    "           # audio_data = self.augmentation(samples=input_audio, sample_rate=config.FS)\n",
    "\n",
    "        #input_model = torch.from_numpy(input_audio).unsqueeze(0).unsqueeze(0) \n",
    "        input_model = torch.from_numpy(input_audio).unsqueeze(0)\n",
    "    \n",
    "        bird_name=row_metadata.primary_label\n",
    "        # target\n",
    "        target = row_metadata.target\n",
    "        \n",
    "        return input_model, torch.tensor(target, dtype=torch.long),bird_name\n",
    "\n",
    "print_date_with_message('dataset compiled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T11:50:45.684201Z",
     "iopub.status.busy": "2025-06-28T11:50:45.683887Z",
     "iopub.status.idle": "2025-06-28T11:50:45.689075Z",
     "shell.execute_reply": "2025-06-28T11:50:45.688260Z",
     "shell.execute_reply.started": "2025-06-28T11:50:45.684168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_transforms(_type):\n",
    "    \n",
    "    if _type == 'train':\n",
    "        return albu.Compose([\n",
    "            albu.HorizontalFlip(0.5),\n",
    "            albu.XYMasking(\n",
    "                p=0.3,\n",
    "                num_masks_x=(1, 3),\n",
    "                num_masks_y=(1, 3),\n",
    "                mask_x_length=(1, 10),\n",
    "                mask_y_length=(1, 20),\n",
    "            ) if config.USE_XYMASKING else albu.NoOp()\n",
    "        ])\n",
    "    elif _type == 'valid':\n",
    "        return albu.Compose([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ici nous vérifions notre dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:29:47.109357Z",
     "iopub.status.busy": "2025-07-03T18:29:47.108265Z",
     "iopub.status.idle": "2025-07-03T18:29:54.966471Z",
     "shell.execute_reply": "2025-07-03T18:29:54.965580Z",
     "shell.execute_reply.started": "2025-07-03T18:29:47.109322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bird records:  24459\n",
      "(1, 160000)\n",
      "tensor(90)\n",
      "insbab1\n",
      "(1, 160000)\n",
      "tensor(39)\n",
      "comkin1\n",
      "(1, 160000)\n",
      "tensor(177)\n",
      "woosan\n",
      "(1, 160000)\n",
      "tensor(129)\n",
      "plapri1\n",
      "(1, 160000)\n",
      "tensor(57)\n",
      "eurcoo\n",
      "(1, 160000)\n",
      "tensor(41)\n",
      "commyn\n",
      "(1, 160000)\n",
      "tensor(177)\n",
      "woosan\n",
      "(1, 160000)\n",
      "tensor(76)\n",
      "grywag\n",
      "(1, 160000)\n",
      "tensor(177)\n",
      "woosan\n",
      "(1, 160000)\n",
      "tensor(53)\n",
      "eaywag1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64665"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "dummy_dataset = BirdDataset(train_df,mode='train',augmentation=None)\n",
    "len_dataset=len(dummy_dataset)\n",
    "print('number of bird records: ',len_dataset)\n",
    "num_samples=10\n",
    "\n",
    "indices = random.sample(range(0, len_dataset), num_samples)\n",
    "\n",
    "for idx in indices:\n",
    "\n",
    "    test_input, test_target,bird_name = dummy_dataset[idx]\n",
    "    print(test_input.detach().numpy().shape)\n",
    "    print(test_target)\n",
    "    print(bird_name)\n",
    "\n",
    "del dummy_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "récupération des valeurs min max de l'audio obsolete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T07:15:22.608845Z",
     "iopub.status.busy": "2025-07-02T07:15:22.608276Z",
     "iopub.status.idle": "2025-07-02T07:52:15.348666Z",
     "shell.execute_reply": "2025-07-02T07:52:15.347460Z",
     "shell.execute_reply.started": "2025-07-02T07:15:22.608818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "sys.exit('lenghty code will exit')\n",
    "dummy_dataset = BirdDataset(train_df,mode='train')\n",
    "len_dataset=len(dummy_dataset)\n",
    "print('number of bird records: ',len_dataset)\n",
    "#num_samples=10\n",
    "\n",
    "#indices = random.sample(range(0, len_dataset), num_samples)\n",
    "max_l=0\n",
    "min_l=1e30\n",
    "for idx in range(len(dummy_dataset)):\n",
    "\n",
    "    test_input, test_target,bird_name = dummy_dataset[idx]\n",
    "    #print(test_input.detach().numpy().shape)\n",
    "    lenght_audio=test_input.detach().numpy().shape[2]\n",
    "    #print(lenght_audio)\n",
    "    if lenght_audio>max_l:\n",
    "        max_l=lenght_audio\n",
    "    if lenght_audio<min_l:\n",
    "        min_l=lenght_audio\n",
    "    if idx%300==0:\n",
    "        print(idx)\n",
    "\n",
    "del dummy_dataset\n",
    "gc.collect()\n",
    "assert max_l!=0\n",
    "assert min_l!=1e30\n",
    "print('max duration: ',max_l)\n",
    "print('min duration: ',min_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:51:36.178498Z",
     "iopub.status.busy": "2025-07-03T18:51:36.178171Z",
     "iopub.status.idle": "2025-07-03T18:51:36.190321Z",
     "shell.execute_reply": "2025-07-03T18:51:36.189208Z",
     "shell.execute_reply.started": "2025-07-03T18:51:36.178473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-03 - models compiled\n"
     ]
    }
   ],
   "source": [
    "class LearnableSpectrogram(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_filters=256, kernel_size=256, stride=128, apply_log=True):\n",
    "        super(LearnableSpectrogram, self).__init__()\n",
    "        self.apply_log = apply_log\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels, n_filters, kernel_size, stride=stride)\n",
    "        self.bn = nn.BatchNorm1d(n_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch, time)\n",
    "        Returns: Tensor of shape (batch, 256, 256)\n",
    "        \"\"\"\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(1)  # (batch, 1, time)\n",
    "\n",
    "        x = self.conv(x)       # (batch, 256, time_frames)\n",
    "        x = torch.abs(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        if self.apply_log:\n",
    "            x = torch.log1p(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EffNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_type, n_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if model_type == 'efficientnet_b0':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B0_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b0(weights=weights)\n",
    "        elif model_type == 'efficientnet_b1':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B1_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b1(weights=weights)\n",
    "        elif model_type == 'efficientnet_b2':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B2_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b2(weights=weights)\n",
    "        elif model_type == 'efficientnet_b3':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B3_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b3(weights=weights)\n",
    "        else:\n",
    "            raise ValueError('model type not supported')\n",
    "        \n",
    "        self.base_model.classifier[1] = nn.Linear(self.base_model.classifier[1].in_features, n_classes, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.cat([x, x, x], dim=3).permute(0, 3, 1, 2)\n",
    "        return self.base_model(x)\n",
    "\n",
    "print_date_with_message('models compiled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "signal processing train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model by PyTorch-Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:51:47.429688Z",
     "iopub.status.busy": "2025-07-03T18:51:47.428979Z",
     "iopub.status.idle": "2025-07-03T18:51:47.444708Z",
     "shell.execute_reply": "2025-07-03T18:51:47.443703Z",
     "shell.execute_reply.started": "2025-07-03T18:51:47.429657Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-03 - class birdmodel compiled\n"
     ]
    }
   ],
   "source": [
    "class BirdModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learnable_process= LearnableSpectrogram()\n",
    "        \n",
    "        # == backbone ==\n",
    "        self.backbone = EffNet(config.MODEL_TYPE, n_classes=len(label_list))\n",
    "        \n",
    "        # == loss function ==\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # == record ==\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, sounds):\n",
    "        images=self.learnable_process(sounds)\n",
    "        return self.backbone(images)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=config.LR,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=config.EPOCHS,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-6,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target,_ = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        y_pred = self(image)\n",
    "        \n",
    "        # == compute loss ==\n",
    "        train_loss = self.loss_fn(y_pred, target)\n",
    "        \n",
    "        # == record ==\n",
    "        self.log('train_loss', train_loss, True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target,_ = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(image)\n",
    "            \n",
    "        self.validation_step_outputs.append({\"logits\": y_pred, \"targets\": target})\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # = merge batch data =\n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        output_val = nn.Softmax(dim=1)(torch.cat([x['logits'] for x in outputs], dim=0)).cpu().detach()\n",
    "        target_val = torch.cat([x['targets'] for x in outputs], dim=0).cpu().detach()\n",
    "        \n",
    "        # = compute validation loss =\n",
    "        val_loss = self.loss_fn(output_val, target_val)\n",
    "        \n",
    "        # target to one-hot\n",
    "        target_val = torch.nn.functional.one_hot(target_val, len(label_list))\n",
    "        \n",
    "        # = val with ROC AUC =\n",
    "        gt_df = pd.DataFrame(target_val.numpy().astype(np.float32), columns=label_list)\n",
    "        pred_df = pd.DataFrame(output_val.numpy().astype(np.float32), columns=label_list)\n",
    "        \n",
    "        gt_df['id'] = [f'id_{i}' for i in range(len(gt_df))]\n",
    "        pred_df['id'] = [f'id_{i}' for i in range(len(pred_df))]\n",
    "        \n",
    "        val_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "        \n",
    "        self.log(\"val_score\", val_score, True)\n",
    "        \n",
    "        # clear validation outputs\n",
    "        self.validation_step_outputs = list()\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_score': val_score}\n",
    "\n",
    "print_date_with_message('class birdmodel compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:34:44.281074Z",
     "iopub.status.busy": "2025-07-03T18:34:44.280747Z",
     "iopub.status.idle": "2025-07-03T18:34:45.846743Z",
     "shell.execute_reply": "2025-07-03T18:34:45.845913Z",
     "shell.execute_reply.started": "2025-07-03T18:34:44.281050Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 182])\n",
      "number of bird records:  24459\n",
      "learn signal layer shape:  torch.Size([1, 256, 1249])\n",
      "side by side output torch.Size([1, 182])\n",
      "birdmodel output torch.Size([1, 182])\n",
      "learn signal layer shape:  torch.Size([1, 256, 1249])\n",
      "side by side output torch.Size([1, 182])\n",
      "birdmodel output torch.Size([1, 182])\n",
      "learn signal layer shape:  torch.Size([1, 256, 1249])\n",
      "side by side output torch.Size([1, 182])\n",
      "birdmodel output torch.Size([1, 182])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model = EffNet(config.MODEL_TYPE, n_classes=len(label_list))\n",
    "learn_signal=LearnableSpectrogram()\n",
    "\n",
    "bird_model=BirdModel()\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(1, 256, 256)\n",
    "print(dummy_model(dummy_input).shape)\n",
    "\n",
    "\n",
    "import random\n",
    "dummy_dataset = BirdDataset(train_df,mode='train',augmentation=None)\n",
    "len_dataset=len(dummy_dataset)\n",
    "print('number of bird records: ',len_dataset)\n",
    "num_samples=3\n",
    "\n",
    "indices = random.sample(range(0, len_dataset), num_samples)\n",
    "\n",
    "for idx in indices:\n",
    "\n",
    "    test_input, test_target,bird_name = dummy_dataset[idx]\n",
    "    image=learn_signal(test_input)\n",
    "    print('learn signal layer shape: ',image.shape)\n",
    "    pred=dummy_model(image)\n",
    "    print('side by side output',pred.shape)\n",
    "    pred_2=bird_model(test_input)\n",
    "    print('birdmodel output',pred_2.shape)\n",
    "   \n",
    "\n",
    "del dummy_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions of Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:51:56.148064Z",
     "iopub.status.busy": "2025-07-03T18:51:56.147695Z",
     "iopub.status.idle": "2025-07-03T18:51:56.155850Z",
     "shell.execute_reply": "2025-07-03T18:51:56.154696Z",
     "shell.execute_reply.started": "2025-07-03T18:51:56.148035Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-03 - predict function compiled\n"
     ]
    }
   ],
   "source": [
    "def predict(data_loader, model):\n",
    "    model.to(config.DEVICE)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            x, y,_= batch\n",
    "            x = x.cuda()\n",
    "            outputs = model(x)\n",
    "            outputs = nn.Softmax(dim=1)(outputs)\n",
    "        assert outputs is not None, \"model inputs is none\"\n",
    "        predictions.append(outputs.detach().cpu())\n",
    "        gts.append(y.detach().cpu())\n",
    "    \n",
    "    predictions = torch.cat(predictions, dim=0).cpu().detach()\n",
    "    gts = torch.cat(gts, dim=0).cpu().detach()\n",
    "    gts = torch.nn.functional.one_hot(gts, len(label_list))\n",
    "    \n",
    "    return predictions.numpy().astype(np.float32), gts.numpy().astype(np.float32)\n",
    "\n",
    "print_date_with_message('predict function compiled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMMENTED LINE WITH SCORE!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:53:30.669472Z",
     "iopub.status.busy": "2025-07-03T18:53:30.668799Z",
     "iopub.status.idle": "2025-07-03T18:53:30.681868Z",
     "shell.execute_reply": "2025-07-03T18:53:30.680981Z",
     "shell.execute_reply.started": "2025-07-03T18:53:30.669444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-03 - run_training function compiled \n"
     ]
    }
   ],
   "source": [
    "def run_training(fold_id, total_df):\n",
    "    print('================================================================')\n",
    "    print(f\"==== Running training for fold {fold_id} ====\")\n",
    "    \n",
    "    # == create dataset and dataloader ==\n",
    "    train_df = total_df[total_df['fold'] != fold_id].copy()\n",
    "    valid_df = total_df[total_df['fold'] == fold_id].copy()\n",
    "    \n",
    "    print(f'Train Samples: {len(train_df)}')\n",
    "    print(f'Valid Samples: {len(valid_df)}')\n",
    "\n",
    "    train_ds = BirdDataset(train_df)\n",
    "    val_ds = BirdDataset(valid_df)\n",
    "\n",
    "    # for now no data augmentation implemented\n",
    "    #train_ds = BirdDataset(train_df, get_transforms('train'), 'train')\n",
    "    #val_ds = BirdDataset(valid_df, get_transforms('valid'), 'valid')\n",
    "    \n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.N_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=config.BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=config.N_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    print('data loaded succesfully')\n",
    "    \n",
    "    # == init model ==\n",
    "    bird_model = BirdModel()\n",
    "\n",
    "    print('bird_model loaded')\n",
    "    \n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_score',\n",
    "                                          dirpath=config.OUTPUT_DIR,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=False,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}\",\n",
    "                                          mode='max')\n",
    "    callbacks_to_use = [checkpoint_callback, TQDMProgressBar(refresh_rate=1)]\n",
    "    \n",
    "    # == init trainer ==\n",
    "    trainer = pl.Trainer(\n",
    "        enable_progress_bar=True,        # Enables rich progress bar\n",
    "        \n",
    "        log_every_n_steps=10,\n",
    "        max_epochs=config.EPOCHS,\n",
    "        val_check_interval=0.5,\n",
    "        callbacks=callbacks_to_use,\n",
    "        enable_model_summary=True,\n",
    "        accelerator=\"gpu\",\n",
    "        deterministic=True,\n",
    "        precision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "    )\n",
    "    print('Will enter training')\n",
    "    # == Training ==\n",
    "    trainer.fit(bird_model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "    \n",
    "    # == Prediction ==\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    weights = torch.load(best_model_path)['state_dict']\n",
    "    bird_model.load_state_dict(weights)\n",
    "    \n",
    "    preds, gts = predict(val_dl, bird_model)\n",
    "    \n",
    "    # = create dataframe =\n",
    "    pred_df = pd.DataFrame(preds, columns=label_list)\n",
    "    pred_df['id'] = np.arange(len(pred_df))\n",
    "    gt_df = pd.DataFrame(gts, columns=label_list)\n",
    "    gt_df['id'] = np.arange(len(gt_df))\n",
    "    \n",
    "    # = compute score =\n",
    "    #val_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "    \n",
    "    # == save to file ==\n",
    "    pred_cols = [f'pred_{t}' for t in label_list]\n",
    "    valid_df = pd.concat([valid_df.reset_index(), pd.DataFrame(np.zeros((len(valid_df), len(label_list)*2)).astype(np.float32), columns=label_list+pred_cols)], axis=1)\n",
    "    valid_df[label_list] = gts\n",
    "    valid_df[pred_cols] = preds\n",
    "    valid_df.to_csv(f\"{config.OUTPUT_DIR}/pred_df_f{fold_id}.csv\", index=False)\n",
    "    \n",
    "    #return preds, gts, val_score\n",
    "    return preds, gts\n",
    "\n",
    "print_date_with_message('run_training function compiled ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:53:34.493455Z",
     "iopub.status.busy": "2025-07-03T18:53:34.492568Z",
     "iopub.status.idle": "2025-07-03T18:53:34.510958Z",
     "shell.execute_reply": "2025-07-03T18:53:34.510043Z",
     "shell.execute_reply.started": "2025-07-03T18:53:34.493425Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfolds generated\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=config.FOLDS, shuffle=True, random_state=config.SEED)\n",
    "train_df['fold'] = 0\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "    train_df.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print('Kfolds generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:53:38.568612Z",
     "iopub.status.busy": "2025-07-03T18:53:38.568249Z",
     "iopub.status.idle": "2025-07-03T19:03:46.285973Z",
     "shell.execute_reply": "2025-07-03T19:03:46.284310Z",
     "shell.execute_reply.started": "2025-07-03T18:53:38.568588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "==== Running training for fold 0 ====\n",
      "Train Samples: 22013\n",
      "Valid Samples: 2446\n",
      "data loaded succesfully\n",
      "bird_model loaded\n",
      "Will enter training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360cdae935744740b8318c1f7208a111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a5f35907384b6b81e05af36afd6abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ParticipantVisibleError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/kaggle/usr/lib/kaggle_metric_utilities/kaggle_metric_utilities.py:55\u001b[0m, in \u001b[0;36msafe_call_score\u001b[0;34m(metric_function, solution, submission, **metric_func_kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     score_result \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmetric_func_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:551\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    550\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 551\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    554\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    555\u001b[0m ):\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParticipantVisibleError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m val_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train_df[train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m f]\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# main loop of f-fold\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#val_preds, val_gts, val_score = run_training(f, train_df)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m val_preds, val_gts \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# record\u001b[39;00m\n\u001b[1;32m     25\u001b[0m oof_df\u001b[38;5;241m.\u001b[39mloc[val_idx, label_list] \u001b[38;5;241m=\u001b[39m val_gts\n",
      "Cell \u001b[0;32mIn[49], line 69\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(fold_id, total_df)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWill enter training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# == Training ==\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbird_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# == Prediction ==\u001b[39;00m\n\u001b[1;32m     72\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m checkpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:141\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_accumulate():\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_first_loop_iter \u001b[38;5;241m=\u001b[39m first_loop_iter\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:142\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:254\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_evaluation_epoch_end()\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_evaluation_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m logged_outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs, []  \u001b[38;5;66;03m# free memory\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:334\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    333\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, hook_name)\n\u001b[0;32m--> 334\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[44], line 107\u001b[0m, in \u001b[0;36mBirdModel.on_validation_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m gt_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(gt_df))]\n\u001b[1;32m    105\u001b[0m pred_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pred_df))]\n\u001b[0;32m--> 107\u001b[0m val_score \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_id_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_score, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# clear validation outputs\u001b[39;00m\n",
      "File \u001b[0;32m/kaggle/usr/lib/birdclef-roc-auc/metric.py:35\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(solution, submission, row_id_column_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m scored_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(solution_sums[solution_sums \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scored_columns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkaggle_metric_utilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_call_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolution\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscored_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscored_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/kaggle/usr/lib/kaggle_metric_utilities/kaggle_metric_utilities.py:64\u001b[0m, in \u001b[0;36msafe_call_score\u001b[0;34m(metric_function, solution, submission, **metric_func_kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m treat_as_participant_error(error_message, solution):\n\u001b[0;32m---> 64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParticipantVisibleError(error_message)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[0;31mParticipantVisibleError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# training\n",
    "# we reduced batch size from 32 to 4 to avoid cuda out of memory error\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# record\n",
    "fold_val_score_list = list()\n",
    "oof_df = train_df.copy()\n",
    "pred_cols = [f'pred_{t}' for t in label_list]\n",
    "oof_df = pd.concat([oof_df, pd.DataFrame(np.zeros((len(oof_df), len(pred_cols)*2)).astype(np.float32), columns=label_list+pred_cols)], axis=1)\n",
    "\n",
    "for f in range(config.FOLDS):\n",
    "    \n",
    "    # get validation index\n",
    "    val_idx = list(train_df[train_df['fold'] == f].index)\n",
    "    \n",
    "    # main loop of f-fold\n",
    "    #val_preds, val_gts, val_score = run_training(f, train_df)\n",
    "    val_preds, val_gts = run_training(f, train_df)\n",
    "    \n",
    "    # record\n",
    "    oof_df.loc[val_idx, label_list] = val_gts\n",
    "    oof_df.loc[val_idx, pred_cols] = val_preds\n",
    "    #fold_val_score_list.append(val_score)\n",
    "    \n",
    "    # only training one fold\n",
    "    break\n",
    "\n",
    "\n",
    "#for idx, val_score in enumerate(fold_val_score_list):\n",
    "   # print(f'Fold {idx} Val Score: {val_score:.5f}')\n",
    "\n",
    "# oof_gt_df = oof_df[['samplename'] + label_list].copy()\n",
    "# oof_pred_df = oof_df[['samplename'] + pred_cols].copy()\n",
    "# oof_pred_df.columns = ['samplename'] + label_list\n",
    "# oof_score = score(oof_gt_df, oof_pred_df, 'samplename')\n",
    "# print(f'OOF Score: {oof_score:.5f}')\n",
    "\n",
    "oof_df.to_csv(f\"{config.OUTPUT_DIR}/oof_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entrainement de la couche signal process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:34:44.281074Z",
     "iopub.status.busy": "2025-07-03T18:34:44.280747Z",
     "iopub.status.idle": "2025-07-03T18:34:45.846743Z",
     "shell.execute_reply": "2025-07-03T18:34:45.845913Z",
     "shell.execute_reply.started": "2025-07-03T18:34:44.281050Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 182])\n",
      "number of bird records:  24459\n",
      "learn signal layer shape:  torch.Size([1, 256, 1249])\n",
      "side by side output torch.Size([1, 182])\n",
      "birdmodel output torch.Size([1, 182])\n",
      "learn signal layer shape:  torch.Size([1, 256, 1249])\n",
      "side by side output torch.Size([1, 182])\n",
      "birdmodel output torch.Size([1, 182])\n",
      "learn signal layer shape:  torch.Size([1, 256, 1249])\n",
      "side by side output torch.Size([1, 182])\n",
      "birdmodel output torch.Size([1, 182])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def oog2spec_via_scipy(audio_data):\n",
    "    # handles NaNs\n",
    "    mean_signal = np.nanmean(audio_data)\n",
    "    audio_data = np.nan_to_num(audio_data, nan=mean_signal) if np.isnan(audio_data).mean() < 1 else np.zeros_like(audio_data)\n",
    "    \n",
    "    # to spec.\n",
    "    frequencies, times, spec_data = sci_signal.spectrogram(\n",
    "        audio_data, \n",
    "        fs=config.FS, \n",
    "        nfft=config.N_FFT, \n",
    "        nperseg=config.WIN_SIZE, \n",
    "        noverlap=config.WIN_LAP, \n",
    "        window='hann'\n",
    "    )\n",
    "    \n",
    "    # Filter frequency range\n",
    "    valid_freq = (frequencies >= config.MIN_FREQ) & (frequencies <= config.MAX_FREQ)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # Log\n",
    "    spec_data = np.log10(spec_data + 1e-20)\n",
    "    \n",
    "    # min/max normalize\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "    \n",
    "    return spec_data\n",
    "learn_signal=LearnableSpectrogram()\n",
    "\n",
    "dummy_dataset = BirdDataset(train_df,mode='train',augmentation=None)\n",
    "len_dataset=len(dummy_dataset)\n",
    "print('number of bird records: ',len_dataset)\n",
    "num_samples=1\n",
    "\n",
    "indices = random.sample(range(0, len_dataset), num_samples)\n",
    "\n",
    "for idx in indices:\n",
    "\n",
    "    test_input, test_target,bird_name = dummy_dataset[idx]\n",
    "    image=learn_signal(test_input)\n",
    "    print('learn signal layer shape: ',image.shape)\n",
    "    \n",
    "   \n",
    "\n",
    "del dummy_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit('must not be executed')\n",
    "\n",
    "class Config():\n",
    "    \n",
    "    # Sample Rate\n",
    "    FS = 32000\n",
    "    \n",
    "    # make sure the spec. for each 5s audio data is [512, 512]\n",
    "    N_FFT = 1095  # N FFT\n",
    "    WIN_SIZE = 412  # WIN Size\n",
    "    WIN_LAP = 100  # OVERLAP\n",
    "    # min frequency\n",
    "    MIN_FREQ = 40\n",
    "    # max frequency\n",
    "    MAX_FREQ = 15000\n",
    "    # Competition Root Folder\n",
    "    ROOT_FOLDER = '/kaggle/input/birdclef-2024'\n",
    "    \n",
    "CONFIG = Config()\n",
    "\n",
    "train_metadata_df = pd.read_csv(\n",
    "        '/kaggle/input/birdclef-2024/train_metadata.csv',\n",
    "        dtype={\n",
    "            'secondary_labels': 'string',\n",
    "            'primary_label': 'category',\n",
    "        },\n",
    "    )\n",
    "\n",
    "# Convert secondary_labels to iterable tuple\n",
    "def parse_secondary_labels(s):\n",
    "    s = s.strip(\"[']\")\n",
    "    s = s.split(\"', '\")\n",
    "    return tuple([e for e in s if len(e) > 0])\n",
    "\n",
    "train_metadata_df['secondary_labels'] = train_metadata_df['secondary_labels'].apply(parse_secondary_labels)\n",
    "\n",
    "# Number of samples\n",
    "CONFIG.N_SAMPLES = len(train_metadata_df)\n",
    "print(f'# Samples: {CONFIG.N_SAMPLES:,}')\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LearnableSpectrogram(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_filters=64, kernel_size=256, stride=128, apply_log=True):\n",
    "        super(LearnableSpectrogram, self).__init__()\n",
    "        self.apply_log = apply_log\n",
    "        \n",
    "        # Convolutional filterbank (learnable)\n",
    "        self.conv = nn.Conv1d(in_channels, n_filters, kernel_size, stride=stride)\n",
    "\n",
    "        # Optional batch norm\n",
    "        self.bn = nn.BatchNorm1d(n_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, time) → assumes mono audio waveform\n",
    "        returns: (batch, filters, time_frames)\n",
    "        \"\"\"\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(1)  # (batch, 1, time)\n",
    "\n",
    "        x = self.conv(x)          # (batch, n_filters, time_frames)\n",
    "        x = torch.abs(x)          # Magnitude of response\n",
    "        x = self.bn(x)\n",
    "\n",
    "        if self.apply_log:\n",
    "            x = torch.log1p(x)    # Log-magnitude\n",
    "\n",
    "        return x\n",
    "model=LearnableSpectrogram()\n",
    "model.eval()\n",
    "\n",
    "test_audio_row = train_metadata_df.iloc[0]\n",
    "test_file = f'{CONFIG.ROOT_FOLDER}/train_audio/{test_audio_row.filename}'\n",
    "print(test_file)\n",
    "\n",
    "# load file\n",
    "audio_data, sample_rate = librosa.load(test_file, sr=CONFIG.FS)\n",
    "print(audio_data.shape, sample_rate)\n",
    "input_model = torch.from_numpy(audio_data).unsqueeze(0).unsqueeze(0) \n",
    "#input_model=torch.from_numpy(audio_data)\n",
    "print(input_model.shape)\n",
    "outputs=model(input_model)\n",
    "print(outputs.shape)\n",
    "\n",
    "display(train_metadata_df.head())\n",
    "display(train_metadata_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# retrieve one sample"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "datasetId": 4776799,
     "sourceId": 8090934,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 167220511,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 189366517,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "complex_net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
